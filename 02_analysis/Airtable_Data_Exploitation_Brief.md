# SCF Airtable Data Exploitation Brief
**Author**: Jay Gutierrez | **Date**: February 24, 2026
**Purpose**: Translate the three downloaded Airtable CSVs into concrete advantages across A5, A6, A7, A9, and A10

---

## What We Have

Three CSVs exported from the SCF Statistics Airtable base (`app8tLjMIDrjeloWN`), which Jay owns:

| File | Rows | Key Signal |
|---|---|---|
| `Awarded Projects [Build only]-By Active _ Category.csv` | 579 | Project canonical identity, GitHub org URLs, funding size, category, Soroban flag |
| `Awarded Submissions [Build only]-By Round.csv` | 758 | Submission-level GitHub repo URLs, technical architecture, team members, tranche completion |
| `Build Award Rounds-By Year.csv` | 48 | Round timing, vote windows, voter counts, award totals |

---

## What the Data Reveals

### The Ecosystem at a Glance

The SCF has distributed **$52.9M** across 579 projects over 48 rounds (40 completed). The active funding universe that matters to PG Atlas is:

- **365 projects** currently at Mainnet, Development, or Expansion status
- **271 of those have GitHub URLs** — the candidate universe for the dependency graph
- **86 are primary PG targets** (Developer Tooling + Infrastructure, active, Soroban, GitHub URL present)
- **338 submission-level GitHub repos** extracted for the git log parser seed list

The Soroban ecosystem is well-represented: 387 of 579 projects (67%) have the Soroban flag. Of the 86 PG seed candidates, 100% are Soroban projects by definition of our filter.

### Category Distribution of the 86 PG Seeds

| Category | Count | % of Seed |
|---|---|---|
| Developer Tooling | ~55 | ~64% |
| Infrastructure & Services | ~31 | ~36% |

### Funding Concentration Risk (Pre-A9 Signal)

Before running a single criticality score, the funding data already hints at structural concentration:

- Top 5 projects by funding represent ~35% of total PG seed budget
- **Reflector** ($444K) and **AnChain.AI** ($435K) dwarf the median (~$70K)
- 22 of the 86 seed projects received exactly $150K — the apparent cap for a single round

This concentration will likely translate directly into high pony-factor risk flags once A9 runs. Reflector and AnChain.AI should be the first test cases when validating the criticality + pony factor pipeline.

### Repeat Contributors — Pre-Seeding the `Contributor` Vertex

The team-member field in submissions yielded 715 distinct contributor names. Crucially, **prolific multi-submission contributors already surface**:

| Contributor | Submissions |
|---|---|
| Christian Rogobete | 13 |
| Ignacio Garcia Leon | 8 |
| Enzo Soyer | 7 |
| Esteban Felipe Iglesias Manríquez | 7 |
| OrbitLens | 6 |
| Christos Salaforis | 6 |

These names are ecosystem-wide prolific contributors. Cross-referencing them against git log data in A7 will produce the first high-confidence `Contributor` vertex seeds — people who appear in both the SCF application records AND actual git commit history. That intersection is analytically powerful for the NQG reputation score.

### Technical Architecture Strings — Package Ecosystem Fingerprinting

The `Technical Architecture` and `Products & Services` fields in the submissions table are free-text but rich. A fast regex pass can extract:
- npm package names from `npm install`, `npm i`, or `@org/package` patterns
- Cargo crate names from `cargo add` or `[dependencies]` blocks
- PyPI package names from `pip install` patterns
- GitHub repo URLs embedded in architecture descriptions

This is not a primary pipeline, but it gives a shortcut: for projects where deps.dev has no PURL coverage (e.g., brand-new packages with zero downloads), the architecture text may be the only way to discover the dependency relationship before an SBOM is generated.

---

## How Each Deliverable Exploits This Data

### A5 — OpenGrants Project Bootstrapper (Alex, but Jay's graph depends on it)

**Input from Airtable**: `01_data/processed/A5_pg_candidate_seed_list.csv` (86 rows)

This is the **ground truth project list** for the initial `Project` vertex population. Alex's A5 deliverable is the OpenGrants pipeline, but the Airtable CSV is the validation layer — every project in the OpenGrants feed should map to a row in this seed list. The `github_url` field in the CSV is the join key.

**Specific exploitation**:
- Use `title` + `github_url` as the composite identity check when A5 pulls OpenGrants data
- The `total_awarded_usd` field provides the funding tier metadata for each Project vertex (`funding_amount` property)
- The `integration_status` field maps directly to PG Atlas `activity_status`: Mainnet → `live`, Development → `in-dev`
- Any OpenGrants project NOT in this CSV is a new project added after the Airtable export (Feb 24, 2026) — worth flagging

**Critical gap**: 8 of the ~94 theoretically qualifying projects had no GitHub URL and were excluded. For those, A5 will need to infer repo URLs from project metadata or website domain matching.

### A6 — Active Subgraph Projection (Jay co-leads)

**Input from Airtable**: `01_data/processed/A6_github_orgs_seed.csv` (78 GitHub orgs)

The 78 distinct GitHub organizations extracted from the seed list are the **root node set** for the registry crawler phase of A6. For each org, the crawler should:
1. Call GitHub API to list all repos under the org
2. For each repo, look up the repo in deps.dev via PURL
3. Construct `ExternalRepo` vertices and `depends_on` edges

**Insight from the data**: Several orgs appear multiple times (Soneso: 2 projects, BP Ventures: 2, Inferara: 2). This means those GitHub orgs are umbrella organizations — the crawler needs to discover ALL repos in those orgs, not just the ones named in the seed list.

**The `orgs` entry with 2 projects**: Two seed list entries have `github.com/orgs/...` URL patterns instead of pointing to a specific org or repo. These will fail PURL lookup and need manual resolution (likely linking to org profile pages rather than code repos).

**Intersection check**: Cross-reference the 78 orgs against known Soroban package publishers on crates.io — this identifies which orgs need the crates.io reverse-dep crawl vs. which can rely on deps.dev alone.

### A7 — Git Log Parser & Contributor Statistics (Jay primary)

**Input from Airtable**: `01_data/processed/A7_submission_github_repos.csv` (338 repos)

This file is the **initial repo run list** for the git log parser. Each row has a GitHub URL, submission title, funding amount, Soroban flag, and tranche completion status.

**Exploitation strategy**:

1. **Prioritize by funding tier**: The top-funded repos are the highest-value for the Metric Gate. Run git log on the top 50 by `total_awarded_usd` first — these will anchor the first credibility check before T2 closes.

2. **Filter by tranche completion**: Repos from submissions with `Tranche Completion = Pre-Launch #3 - Mainnet` (93 submissions) have the most historical commit data. These are the richest source of `contributed_to` edges.

3. **Pre-validate GitHub URLs**: Several submission GitHub URLs are malformed (trailing path segments like `/commits/main/`, tree paths, or doc links). Run URL normalization before passing to the git log parser — extract just the `org/repo` component.

4. **Team member cross-reference**: After parsing git logs, join the extracted committer emails/usernames against the 715 team member names in the submissions table. This validates that the git attribution aligns with the stated team — discrepancies are data quality flags.

5. **Dormancy signal**: The `Most Recent Payment Date` field in submissions gives you a proxy for "when did this project's funding period end." Repos with payment dates > 12 months ago and no commits since are likely candidates for `activity_status = inactive`, even if they show as Mainnet in the Airtable.

### A9 — Criticality Scores + Pony Factor (Jay primary)

**Input from Airtable**: The funding amounts provide a **sanity check calibration layer** for criticality scores.

The hypothesis to test: do the highest-funded projects correlate with the highest criticality scores? If Reflector ($444K) and AnChain.AI ($435K) don't score highly on transitive dependent count, that's analytically interesting — it means the community is funding projects based on factors the dependency graph doesn't capture (e.g., brand, team reputation, market position). If they DO score highly, it validates both the SCF's funding decisions and PG Atlas's scoring methodology simultaneously.

**Pony factor bootstrapping**: The 15 prolific contributors (13–4 submissions each) are the prior probability layer for "this person is likely the primary committer" in their respective repos. Seed the pony factor calculation with the hypothesis that Christian Rogobete, OrbitLens, etc. are high-pony-factor contributors, then verify against git log data.

**Risk flag pre-identification**: Projects with `total_awarded_usd > $200K` and `integration_status = Development` (still in dev despite large funding) should be auto-flagged as high-risk when pony factor = 1. This is the most actionable combination.

### A10 — Adoption Signals (Jay primary)

**Input from Airtable**: The `Website` field from the projects table provides alternative adoption signal endpoints.

Where a project has a website (not just GitHub), that domain may have:
- npm package published under the org's name
- PyPI package discoverable via the org's GitHub presence
- Documentation pages with install count indicators

The 86-project seed list gives the baseline population for A10's percentile normalization. When computing `adoption_downloads` percentile rank, the rank is computed across this population — not against all of npm/PyPI/crates.io globally.

**Soroban-specific gap**: Many Soroban contracts are not published to any registry — they're deployed directly on-chain. For these, `adoption_downloads` will be zero and the adoption signal must come entirely from `adoption_stars` and `adoption_forks` (both available via deps.dev `GetProject`). The Airtable data lets us pre-identify which projects are likely pure on-chain (no registry presence) vs. SDK/tooling projects that will have download counts.

**Download signal proxy**: The `Traction Evidence` field in submissions (not in the projects table but in the subs table) occasionally contains explicit user/download metrics cited by applicants. A regex pass on this field could yield pre-A10 adoption signal estimates — useful for sanity-checking the registry API numbers.

---

## Data Quality Issues to Know Before Implementation

| Issue | Impact | Mitigation |
|---|---|---|
| 8 PG seed candidates missing GitHub URL | Can't include in graph bootstrap | Manual resolution or OpenGrants fallback |
| Malformed GitHub URLs in submissions (trailing paths) | A7 git log parser will fail | URL normalization step pre-processing |
| `Technical Architecture` often links to Google Docs (JS-rendered) | Can't scrape for package names | Skip; use SBOM webhook as primary signal |
| Team member names in free-text (comma/semicolon delimited) | Messy for contributor matching | Fuzzy match against GitHub committer names |
| Soroban? column is `checked`/blank (not boolean) | Filter logic must account for this | Already handled in extraction scripts |
| `Total Awarded` has inconsistent formatting ($, commas) | Money parsing fails | Regex stripping already implemented |
| GitLab URL in seed list (InfStones) | deps.dev and GitHub API won't reach it | Flag as `ExternalRepo` with manual enrichment |

---

## Processed Files Reference

All extraction scripts were run on 2026-02-24. Files in `01_data/processed/`:

| File | Rows | Derivation |
|---|---|---|
| `A5_pg_candidate_seed_list.csv` | 86 | Projects: Developer Tooling or Infra, active status, Soroban=checked, GitHub URL present |
| `A5_all_active_with_github.csv` | 271 | Projects: any active status, GitHub URL present (broader background graph) |
| `A6_github_orgs_seed.csv` | 78 | Distinct GitHub orgs extracted from A5 seed list URLs |
| `A7_submission_github_repos.csv` | 338 | Submissions with GitHub URL in Code field |

---

## Recommended Immediate Actions (Pre-T1 Closure, before March 8)

1. **Share `A5_pg_candidate_seed_list.csv` with Alex** — this is the validation layer he needs to check A5 OpenGrants output against. If OpenGrants returns < 86 projects, the gap rows need manual attention.

2. **Run URL normalization on `A7_submission_github_repos.csv`** — extract canonical `{owner}/{repo}` from all 338 URLs and flag the malformed ones. This is a 30-minute Python task that removes a silent failure mode in A7.

3. **Identify the 8 "missing GitHub" PG candidates** — cross-reference against the full 579-project list to find which ones have websites but no GitHub. Bring to Alex for OpenGrants resolution path.

4. **Propose using this seed list as the A6 crawler entrypoint to Alex** — the 78-org list gives a concrete, bounded starting set rather than an open-ended crawl. This anchors the BFS scope and makes the active subgraph projection tractable within T2.

---

*Document prepared February 24, 2026 | Jay Gutierrez*
*Sources: SCF Airtable base app8tLjMIDrjeloWN (Owner: Jay), Python extraction scripts run on downloaded CSVs*
